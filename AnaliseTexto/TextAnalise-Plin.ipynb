{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "import numpy as np \n",
    "from nltk import word_tokenize\n",
    "from PyPDF2 import PdfFileReader \n",
    "\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0f60107105e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0murllib2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mStringIO\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'my_url'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "from urllib2 import Request\n",
    "from StringIO import StringIO\n",
    "\n",
    "url = 'my_url'\n",
    "\n",
    "open = urllib2.urlopen(Request(url)).read()\n",
    "memoryFile = StringIO(open)\n",
    "\n",
    "parser = PDFParser(memoryFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a9e7b8b557b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sandeco/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \"\"\"\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sandeco/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sandeco/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    765\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "vect.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '1970',\n",
       " '2016',\n",
       " '20m',\n",
       " '2ddsj3e',\n",
       " '2diuymo',\n",
       " '2djinud',\n",
       " '2dldkvo',\n",
       " '2duflqf',\n",
       " '2dz5lvp',\n",
       " '2e3gg21',\n",
       " '2ebk4aq',\n",
       " '2ecwika',\n",
       " '2eeuhue',\n",
       " '2eftvdq',\n",
       " '2ei1kky',\n",
       " '2ekekgi',\n",
       " '2en6tba',\n",
       " '2ep1jjk',\n",
       " '2ervn3s',\n",
       " '2erx7ci',\n",
       " '50',\n",
       " '5zv28z',\n",
       " 'about',\n",
       " 'adorei',\n",
       " 'ai',\n",
       " 'alberto',\n",
       " 'algorithms',\n",
       " 'algoritmo',\n",
       " 'an',\n",
       " 'and',\n",
       " 'articles',\n",
       " 'as',\n",
       " 'away',\n",
       " 'baseado',\n",
       " 'become',\n",
       " 'best',\n",
       " 'bigdata',\n",
       " 'blogs',\n",
       " 'brazil',\n",
       " 'brazilian',\n",
       " 'buff',\n",
       " 'by',\n",
       " 'café',\n",
       " 'cam',\n",
       " 'cancer',\n",
       " 'captain',\n",
       " 'carlos',\n",
       " 'central',\n",
       " 'coisas',\n",
       " 'com',\n",
       " 'copão',\n",
       " 'course',\n",
       " 'dadascience',\n",
       " 'das',\n",
       " 'data',\n",
       " 'datafloq',\n",
       " 'datascience',\n",
       " 'datasciencecentral',\n",
       " 'dataviz',\n",
       " 'day',\n",
       " 'de',\n",
       " 'deep',\n",
       " 'descent',\n",
       " 'device',\n",
       " 'easy',\n",
       " 'educate',\n",
       " 'esse',\n",
       " 'everyday',\n",
       " 'experience',\n",
       " 'fantástico',\n",
       " 'feature',\n",
       " 'featuring',\n",
       " 'flip',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'genomic',\n",
       " 'gig',\n",
       " 'gl',\n",
       " 'global',\n",
       " 'goo',\n",
       " 'gradient',\n",
       " 'how',\n",
       " 'http',\n",
       " 'https',\n",
       " 'ibm',\n",
       " 'impact',\n",
       " 'implementing',\n",
       " 'improving',\n",
       " 'in',\n",
       " 'insiders',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'introduction',\n",
       " 'iot',\n",
       " 'it',\n",
       " 'kaggle',\n",
       " 'know',\n",
       " 'languages',\n",
       " 'latest',\n",
       " 'learn',\n",
       " 'learning',\n",
       " 'libraries',\n",
       " 'life',\n",
       " 'ly',\n",
       " 'machine',\n",
       " 'machinelearning',\n",
       " 'make',\n",
       " 'maker',\n",
       " 'matplotlib',\n",
       " 'monitoramento',\n",
       " 'moving',\n",
       " 'na',\n",
       " 'need',\n",
       " 'networks',\n",
       " 'neural',\n",
       " 'of',\n",
       " 'oi',\n",
       " 'optimization',\n",
       " 'overview',\n",
       " 'own',\n",
       " 'passed',\n",
       " 'platform',\n",
       " 'podcasts',\n",
       " 'predictions',\n",
       " 'preparation',\n",
       " 'profiles',\n",
       " 'programming',\n",
       " 'python',\n",
       " 'raises',\n",
       " 'releases',\n",
       " 'ripcapita',\n",
       " 'science',\n",
       " 'scientist',\n",
       " 'scikit',\n",
       " 'selection',\n",
       " 'sequencing',\n",
       " 'service',\n",
       " 'serviço',\n",
       " 'sets',\n",
       " 'simulated',\n",
       " 'smart',\n",
       " 'smartfrog',\n",
       " 'startup',\n",
       " 'steps',\n",
       " 'svm',\n",
       " 'teachers',\n",
       " 'team',\n",
       " 'tensorflow',\n",
       " 'testa',\n",
       " 'text',\n",
       " 'the',\n",
       " 'things',\n",
       " 'this',\n",
       " 'time',\n",
       " 'tips',\n",
       " 'to',\n",
       " 'tools',\n",
       " 'top',\n",
       " 'topics',\n",
       " 'torres',\n",
       " 'treatment',\n",
       " 'tricks',\n",
       " 'tutorial',\n",
       " 'use',\n",
       " 'using',\n",
       " 'variable',\n",
       " 'videos',\n",
       " 'vincent',\n",
       " 'warming',\n",
       " 'watson',\n",
       " 'watsons',\n",
       " 'will',\n",
       " 'with',\n",
       " 'wtvufo',\n",
       " 'www',\n",
       " 'you',\n",
       " 'your',\n",
       " 'youtube']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23x180 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 346 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train_dtm = vect.transform(text)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '1970',\n",
       " '2016',\n",
       " '20m',\n",
       " '2ddsj3e',\n",
       " '2diuymo',\n",
       " '2djinud',\n",
       " '2dldkvo',\n",
       " '2duflqf',\n",
       " '2dz5lvp',\n",
       " '2e3gg21',\n",
       " '2ebk4aq',\n",
       " '2ecwika',\n",
       " '2eeuhue',\n",
       " '2eftvdq',\n",
       " '2ei1kky',\n",
       " '2ekekgi',\n",
       " '2en6tba',\n",
       " '2ep1jjk',\n",
       " '2ervn3s',\n",
       " '2erx7ci',\n",
       " '50',\n",
       " '5zv28z',\n",
       " 'about',\n",
       " 'adorei',\n",
       " 'ai',\n",
       " 'alberto',\n",
       " 'algorithms',\n",
       " 'algoritmo',\n",
       " 'an',\n",
       " 'and',\n",
       " 'articles',\n",
       " 'as',\n",
       " 'away',\n",
       " 'baseado',\n",
       " 'become',\n",
       " 'best',\n",
       " 'bigdata',\n",
       " 'blogs',\n",
       " 'brazil',\n",
       " 'brazilian',\n",
       " 'buff',\n",
       " 'by',\n",
       " 'café',\n",
       " 'cam',\n",
       " 'cancer',\n",
       " 'captain',\n",
       " 'carlos',\n",
       " 'central',\n",
       " 'coisas',\n",
       " 'com',\n",
       " 'copão',\n",
       " 'course',\n",
       " 'dadascience',\n",
       " 'das',\n",
       " 'data',\n",
       " 'datafloq',\n",
       " 'datascience',\n",
       " 'datasciencecentral',\n",
       " 'dataviz',\n",
       " 'day',\n",
       " 'de',\n",
       " 'deep',\n",
       " 'descent',\n",
       " 'device',\n",
       " 'easy',\n",
       " 'educate',\n",
       " 'esse',\n",
       " 'everyday',\n",
       " 'experience',\n",
       " 'fantástico',\n",
       " 'feature',\n",
       " 'featuring',\n",
       " 'flip',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'genomic',\n",
       " 'gig',\n",
       " 'gl',\n",
       " 'global',\n",
       " 'goo',\n",
       " 'gradient',\n",
       " 'how',\n",
       " 'http',\n",
       " 'https',\n",
       " 'ibm',\n",
       " 'impact',\n",
       " 'implementing',\n",
       " 'improving',\n",
       " 'in',\n",
       " 'insiders',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'introduction',\n",
       " 'iot',\n",
       " 'it',\n",
       " 'kaggle',\n",
       " 'know',\n",
       " 'languages',\n",
       " 'latest',\n",
       " 'learn',\n",
       " 'learning',\n",
       " 'libraries',\n",
       " 'life',\n",
       " 'ly',\n",
       " 'machine',\n",
       " 'machinelearning',\n",
       " 'make',\n",
       " 'maker',\n",
       " 'matplotlib',\n",
       " 'monitoramento',\n",
       " 'moving',\n",
       " 'na',\n",
       " 'need',\n",
       " 'networks',\n",
       " 'neural',\n",
       " 'of',\n",
       " 'oi',\n",
       " 'optimization',\n",
       " 'overview',\n",
       " 'own',\n",
       " 'passed',\n",
       " 'platform',\n",
       " 'podcasts',\n",
       " 'predictions',\n",
       " 'preparation',\n",
       " 'profiles',\n",
       " 'programming',\n",
       " 'python',\n",
       " 'raises',\n",
       " 'releases',\n",
       " 'ripcapita',\n",
       " 'science',\n",
       " 'scientist',\n",
       " 'scikit',\n",
       " 'selection',\n",
       " 'sequencing',\n",
       " 'service',\n",
       " 'serviço',\n",
       " 'sets',\n",
       " 'simulated',\n",
       " 'smart',\n",
       " 'smartfrog',\n",
       " 'startup',\n",
       " 'steps',\n",
       " 'svm',\n",
       " 'teachers',\n",
       " 'team',\n",
       " 'tensorflow',\n",
       " 'testa',\n",
       " 'text',\n",
       " 'the',\n",
       " 'things',\n",
       " 'this',\n",
       " 'time',\n",
       " 'tips',\n",
       " 'to',\n",
       " 'tools',\n",
       " 'top',\n",
       " 'topics',\n",
       " 'torres',\n",
       " 'treatment',\n",
       " 'tricks',\n",
       " 'tutorial',\n",
       " 'use',\n",
       " 'using',\n",
       " 'variable',\n",
       " 'videos',\n",
       " 'vincent',\n",
       " 'warming',\n",
       " 'watson',\n",
       " 'watsons',\n",
       " 'will',\n",
       " 'with',\n",
       " 'wtvufo',\n",
       " 'www',\n",
       " 'you',\n",
       " 'your',\n",
       " 'youtube']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab = list(vect.get_feature_names())\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iot', 21),\n",
       " ('http', 19),\n",
       " ('datascience', 18),\n",
       " ('machinelearning', 17),\n",
       " ('bigdata', 17),\n",
       " ('buff', 17),\n",
       " ('ly', 17),\n",
       " ('to', 8),\n",
       " ('the', 7),\n",
       " ('and', 6),\n",
       " ('data', 6),\n",
       " ('an', 5),\n",
       " ('of', 5),\n",
       " ('internet', 4),\n",
       " ('introduction', 4),\n",
       " ('with', 4),\n",
       " ('de', 3),\n",
       " ('things', 3),\n",
       " ('in', 3),\n",
       " ('science', 3),\n",
       " ('neural', 2),\n",
       " ('about', 2),\n",
       " ('networks', 2),\n",
       " ('feature', 2),\n",
       " ('tensorflow', 2),\n",
       " ('ibm', 2),\n",
       " ('variable', 2),\n",
       " ('learning', 2),\n",
       " ('selection', 2),\n",
       " ('youtube', 2),\n",
       " ('your', 2),\n",
       " ('predictions', 2),\n",
       " ('10', 2),\n",
       " ('captain', 2),\n",
       " ('by', 1),\n",
       " ('podcasts', 1),\n",
       " ('tools', 1),\n",
       " ('team', 1),\n",
       " ('text', 1),\n",
       " ('genomic', 1),\n",
       " ('languages', 1),\n",
       " ('esse', 1),\n",
       " ('2diuymo', 1),\n",
       " ('maker', 1),\n",
       " ('libraries', 1),\n",
       " ('learn', 1),\n",
       " ('interview', 1),\n",
       " ('gl', 1),\n",
       " ('scientist', 1),\n",
       " ('café', 1),\n",
       " ('everyday', 1),\n",
       " ('2duflqf', 1),\n",
       " ('cam', 1),\n",
       " ('baseado', 1),\n",
       " ('away', 1),\n",
       " ('device', 1),\n",
       " ('watsons', 1),\n",
       " ('improving', 1),\n",
       " ('programming', 1),\n",
       " ('overview', 1),\n",
       " ('warming', 1),\n",
       " ('2ecwika', 1),\n",
       " ('how', 1),\n",
       " ('own', 1),\n",
       " ('make', 1),\n",
       " ('machine', 1),\n",
       " ('steps', 1),\n",
       " ('kaggle', 1),\n",
       " ('raises', 1),\n",
       " ('svm', 1),\n",
       " ('vincent', 1),\n",
       " ('time', 1),\n",
       " ('python', 1),\n",
       " ('datasciencecentral', 1),\n",
       " ('copão', 1),\n",
       " ('best', 1),\n",
       " ('need', 1),\n",
       " ('datafloq', 1),\n",
       " ('das', 1),\n",
       " ('2erx7ci', 1),\n",
       " ('testa', 1),\n",
       " ('flip', 1),\n",
       " ('become', 1),\n",
       " ('2ekekgi', 1),\n",
       " ('fantástico', 1),\n",
       " ('platform', 1),\n",
       " ('serviço', 1),\n",
       " ('smart', 1),\n",
       " ('scikit', 1),\n",
       " ('tutorial', 1),\n",
       " ('cancer', 1),\n",
       " ('ai', 1),\n",
       " ('top', 1),\n",
       " ('2ei1kky', 1),\n",
       " ('it', 1),\n",
       " ('startup', 1),\n",
       " ('sets', 1),\n",
       " ('2ep1jjk', 1),\n",
       " ('from', 1),\n",
       " ('algoritmo', 1),\n",
       " ('2eftvdq', 1),\n",
       " ('2dz5lvp', 1),\n",
       " ('blogs', 1),\n",
       " ('50', 1),\n",
       " ('easy', 1),\n",
       " ('dataviz', 1),\n",
       " ('further', 1),\n",
       " ('5zv28z', 1),\n",
       " ('central', 1),\n",
       " ('goo', 1),\n",
       " ('topics', 1),\n",
       " ('2e3gg21', 1),\n",
       " ('preparation', 1),\n",
       " ('implementing', 1),\n",
       " ('2eeuhue', 1),\n",
       " ('descent', 1),\n",
       " ('as', 1),\n",
       " ('20m', 1),\n",
       " ('using', 1),\n",
       " ('treatment', 1),\n",
       " ('latest', 1),\n",
       " ('will', 1),\n",
       " ('releases', 1),\n",
       " ('monitoramento', 1),\n",
       " ('https', 1),\n",
       " ('alberto', 1),\n",
       " ('watson', 1),\n",
       " ('ripcapita', 1),\n",
       " ('torres', 1),\n",
       " ('course', 1),\n",
       " ('featuring', 1),\n",
       " ('brazil', 1),\n",
       " ('wtvufo', 1),\n",
       " ('coisas', 1),\n",
       " ('use', 1),\n",
       " ('passed', 1),\n",
       " ('oi', 1),\n",
       " ('optimization', 1),\n",
       " ('moving', 1),\n",
       " ('com', 1),\n",
       " ('know', 1),\n",
       " ('simulated', 1),\n",
       " ('2ervn3s', 1),\n",
       " ('you', 1),\n",
       " ('www', 1),\n",
       " ('this', 1),\n",
       " ('dadascience', 1),\n",
       " ('adorei', 1),\n",
       " ('educate', 1),\n",
       " ('for', 1),\n",
       " ('1970', 1),\n",
       " ('2en6tba', 1),\n",
       " ('teachers', 1),\n",
       " ('matplotlib', 1),\n",
       " ('global', 1),\n",
       " ('sequencing', 1),\n",
       " ('life', 1),\n",
       " ('2ebk4aq', 1),\n",
       " ('insiders', 1),\n",
       " ('gig', 1),\n",
       " ('carlos', 1),\n",
       " ('2016', 1),\n",
       " ('impact', 1),\n",
       " ('day', 1),\n",
       " ('2ddsj3e', 1),\n",
       " ('profiles', 1),\n",
       " ('experience', 1),\n",
       " ('brazilian', 1),\n",
       " ('smartfrog', 1),\n",
       " ('deep', 1),\n",
       " ('gradient', 1),\n",
       " ('na', 1),\n",
       " ('videos', 1),\n",
       " ('service', 1),\n",
       " ('tricks', 1),\n",
       " ('algorithms', 1),\n",
       " ('tips', 1),\n",
       " ('2dldkvo', 1),\n",
       " ('2djinud', 1),\n",
       " ('articles', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = simple_train_dtm.sum(axis=0).A1\n",
    "\n",
    "freq_distribution = Counter(dict(zip(vocab, counts)))\n",
    "##print (freq_distribution.most_common(100))\n",
    "list(freq_distribution.most_common())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
